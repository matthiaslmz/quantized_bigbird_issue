{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 4.9.1\n",
      "Version: 1.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers | grep Version\n",
    "!pip show torch | grep Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import (BigBirdTokenizer,\n",
    "                          BigBirdConfig,\n",
    "                          BigBirdForSequenceClassification)\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 65, 387, 784,  ..., 441, 474,  66]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base') \n",
    "\n",
    "with open(\"string.txt\",\"r\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)[-1536:])\n",
    "encoded_inputs = tokenizer.encode_plus(ids, add_special_tokens = True, max_length=1538, padding='max_length', truncation=True, return_tensors='pt')\n",
    "print(encoded_inputs)\n",
    "\n",
    "input_ids = encoded_inputs['input_ids']\n",
    "attention_mask = encoded_inputs['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:2054: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.attention_type == \"block_sparse\" and seq_length <= max_tokens_to_attend:\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:2227: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if padding_len > 0:\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:2178: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  seq_length % block_size == 0\n",
      "/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:463: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert from_seq_length % from_block_size == 0, \"Query sided sequence length must be multiple of block size\"\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:464: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert to_seq_length % to_block_size == 0, \"Key/Value sided sequence length must be multiple of block size\"\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:560: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if from_seq_len // from_block_size != to_seq_len // to_block_size:\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:569: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if from_seq_len in [1024, 3072, 4096]:  # old plans used in paper\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:1043: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (2 * num_rand_blocks + 5) < (from_seq_length // from_block_size):\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:1157: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  from_seq_length // from_block_size == to_seq_length // to_block_size\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:1160: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert from_seq_length in plan_from_length, \"Error from sequence length not in plan!\"\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:1165: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  plan_block_length = np.array(plan_from_length) // from_block_size\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:1167: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_plan_idx = plan_from_length.index(from_seq_length)\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:593: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  rand_attn = torch.tensor(rand_attn, device=query_layer.device, dtype=torch.long)\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:1021: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  rand_mask = torch.stack([p1[i1.flatten()] for p1, i1 in zip(to_blocked_mask, rand_attn)])\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:967: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if params.shape[:2] != indices.shape[:2]:\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py:2155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:2157: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if padding_len > 0:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0INTERNAL ASSERT FAILED at \"/pytorch/torch/csrc/jit/ir/alias_analysis.cpp\":532, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Argument types: Tensor, int[], bool, ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1dfdd2340788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtraced_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraced_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"traced_bigbird.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0m_module_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         )\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m                 \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m                 \u001b[0margument_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m             )\n\u001b[1;32m    961\u001b[0m             \u001b[0mcheck_trace_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0INTERNAL ASSERT FAILED at \"/pytorch/torch/csrc/jit/ir/alias_analysis.cpp\":532, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Argument types: Tensor, int[], bool, "
     ]
    }
   ],
   "source": [
    "model = BigBirdForSequenceClassification.from_pretrained(\"google/bigbird-roberta-base\", torchscript=True)\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "traced_model = torch.jit.trace(model, (input_ids, attention_mask))\n",
    "torch.jit.save(traced_model, \"traced_bigbird.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import (BertTokenizer,\n",
    "                          BertConfig,\n",
    "                          BertForSequenceClassification)\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "\n",
    "with open(\"string.txt\",\"r\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)[-510:])\n",
    "encoded_inputs = tokenizer.encode_plus(ids, add_special_tokens = True, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "input_ids = encoded_inputs['input_ids']\n",
    "attention_mask = encoded_inputs['attention_mask']\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", torchscript=True)\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "traced_model = torch.jit.trace(model, (input_ids, attention_mask))\n",
    "torch.jit.save(traced_model, \"traced_bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import (RobertaTokenizer,\n",
    "                          RobertaConfig,\n",
    "                          RobertaForSequenceClassification)\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base') \n",
    "\n",
    "with open(\"string.txt\",\"r\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)[-510:])\n",
    "encoded_inputs = tokenizer.encode_plus(ids, add_special_tokens = True, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "print(encoded_inputs['input_ids'], encoded_inputs['attention_mask'])\n",
    "\n",
    "input_ids = encoded_inputs['input_ids']\n",
    "attention_mask = encoded_inputs['attention_mask']\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", torchscript=True)\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "traced_model = torch.jit.trace(model, (input_ids, attention_mask))\n",
    "torch.jit.save(traced_model, \"traced_bert.pt\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
